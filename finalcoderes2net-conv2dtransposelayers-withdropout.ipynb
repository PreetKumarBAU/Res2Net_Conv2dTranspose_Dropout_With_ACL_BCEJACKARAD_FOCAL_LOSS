{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, Reshape, Permute, Activation, Input, \\\n    add, multiply\nfrom keras.layers import concatenate, core, Dropout\nfrom keras.models import Model\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nfrom keras.layers.core import Lambda\n#import keras.backend as K\n\n#%tensorflow_version 1.x\nimport os\nimport keras\nfrom keras.callbacks import TensorBoard\nimport tensorflow as tf\n#import keras.backend.tensorflow_backend as K\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import CSVLogger\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-14T13:11:23.654302Z","iopub.execute_input":"2021-08-14T13:11:23.654653Z","iopub.status.idle":"2021-08-14T13:11:23.662466Z","shell.execute_reply.started":"2021-08-14T13:11:23.654621Z","shell.execute_reply":"2021-08-14T13:11:23.660835Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom glob import glob\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2021-08-14T13:11:23.762467Z","iopub.execute_input":"2021-08-14T13:11:23.762717Z","iopub.status.idle":"2021-08-14T13:11:23.766747Z","shell.execute_reply.started":"2021-08-14T13:11:23.762693Z","shell.execute_reply":"2021-08-14T13:11:23.765686Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nimport numpy as np\nimport os\nimport skimage.io as io\nimport skimage.transform as trans\nimport cv2\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nBackGround = [255, 255, 255]\nroad = [0, 0, 0]\n# COLOR_DICT = np.array([BackGround, road])\none = [128, 128, 128]\ntwo = [128, 0, 0]\nthree = [192, 192, 128]\nfour = [255, 69, 0]\nfive = [128, 64, 128]\nsix = [60, 40, 222]\nseven = [128, 128, 0]\neight = [192, 128, 128]\nnine = [64, 64, 128]\nten = [64, 0, 128]\neleven = [64, 64, 0]\ntwelve = [0, 128, 192]\nCOLOR_DICT = np.array([one, two,three,four,five,six,seven,eight,nine,ten,eleven,twelve])\n\n\nclass data_preprocess:\n    def __init__(self, train_path=None, image_folder=None, label_folder=None,\n                 valid_path=None,valid_image_folder =None,valid_label_folder = None,\n                 test_path=None, save_path=None,\n                 img_rows=256, img_cols=256,\n                 flag_multi_class=False,\n                 num_classes = 2):\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.train_path = train_path\n        self.image_folder = image_folder\n        self.label_folder = label_folder\n        self.valid_path = valid_path\n        self.valid_image_folder = valid_image_folder\n        self.valid_label_folder = valid_label_folder\n        self.test_path = test_path\n        self.save_path = save_path\n        self.data_gen_args = dict(rotation_range=20,\n                                  width_shift_range=0.002,\n                                  shear_range=0.03,\n                                  zoom_range=0.005,\n                                  vertical_flip=True,\n                                  horizontal_flip=True,\n                                  fill_mode='nearest')\n        self.image_color_mode = \"rgb\"\n        self.label_color_mode = \"grayscale\"\n\n        self.flag_multi_class = flag_multi_class\n        self.num_class = num_classes\n        self.target_size = (256, 256)\n        self.img_type = 'png'\n\n    def adjustData(self, img, label):\n        if (self.flag_multi_class):\n            img = img / 255.\n            label = label[:, :, :, 0] if (len(label.shape) == 4) else label[:, :, 0]\n            new_label = np.zeros(label.shape + (self.num_class,))\n            for i in range(self.num_class):\n                new_label[label == i, i] = 1\n            label = new_label\n        elif (np.max(img) > 1):\n            #img = img / 255.\n            #label = label / 255.\n            #label[label >= 0.5] = 1\n            #label[label < 0.5] = 0\n            img2 =np.asarray(img)\n            label2 =np.asarray(label)\n            img2 =img2.astype('float32')\n            label2 =label2.astype('float32')\n            img2 /= 255.0\n            label2 /= 255.0\n            label2[label2 >= 0.5] = 1\n            label2[label2 < 0.5] = 0\n        return (img2, label2)\n\n    def trainGenerator(self, batch_size, image_save_prefix=\"image\", label_save_prefix=\"label\",\n                       save_to_dir=None, seed=7):\n        '''\n        can generate image and label at the same time\n        use the same seed for image_datagen and label_datagen to ensure the transformation for image and label is the same\n        if you want to visualize the results of generator, set save_to_dir = \"your path\"\n        '''\n        image_datagen = ImageDataGenerator(**self.data_gen_args)\n        label_datagen = ImageDataGenerator(**self.data_gen_args)\n        image_generator = image_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=image_save_prefix,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.train_path,\n            classes=[self.label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            save_to_dir=save_to_dir,\n            save_prefix=label_save_prefix,\n            seed=seed)\n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n\n    def testGenerator(self):\n        filenames = os.listdir(self.test_path)\n        for filename in filenames:\n            img = io.imread(os.path.join(self.test_path, filename), as_gray=False)\n            img = img[: , : , :3]\n            img = img / 255.\n            img = trans.resize(img, self.target_size, mode='constant')\n            #img = np.reshape(img, img.shape + (1,)) if (not self.flag_multi_class) else img\n            img = np.reshape(img, (1,) + img.shape)\n            yield img\n\n    def validLoad(self, batch_size,seed=7):\n        image_datagen = ImageDataGenerator(rescale=0)\n        label_datagen = ImageDataGenerator(rescale=0)\n        image_generator = image_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_image_folder],\n            class_mode=None,\n            color_mode=self.image_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)\n        label_generator = label_datagen.flow_from_directory(\n            self.valid_path,\n            classes=[self.valid_label_folder],\n            class_mode=None,\n            color_mode=self.label_color_mode,\n            target_size=self.target_size,\n            batch_size=batch_size,\n            seed=seed)\n        train_generator = zip(image_generator, label_generator)\n        for (img, label) in train_generator:\n            img, label = self.adjustData(img, label)\n            yield (img, label)\n        # return imgs,labels\n        \n    def saveResult(self, npyfile, size, name,threshold=80):\n        for i, item in enumerate(npyfile):\n            img = item\n            #print(img.shape)\n            #break\n            img_std = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n            if self.flag_multi_class:\n                for row in range(len(img)):\n                    for col in range(len(img[row])):\n                        num = np.argmax(img[row][col])\n                        img_std[row][col] = COLOR_DICT[num]\n            else:\n                for k in range(len(img)):\n                    for j in range(len(img[k])):\n                        num = img[k][j]\n                        #print(num)\n                        #break\n                        if num < (threshold/255.0):\n                            img_std[k][j] = road\n                        else:\n                            img_std[k][j] = BackGround\n            #img_std = cv2.resize(img_std, size, interpolation=cv2.INTER_CUBIC)\n            cv2.imwrite(os.path.join(self.save_path, (\"%s_predict.\" + self.img_type) % (name)), img_std)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T13:11:23.771348Z","iopub.execute_input":"2021-08-14T13:11:23.771649Z","iopub.status.idle":"2021-08-14T13:11:23.805531Z","shell.execute_reply.started":"2021-08-14T13:11:23.771613Z","shell.execute_reply":"2021-08-14T13:11:23.804393Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"####  Metrics\n\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy\nimport tensorflow as tf\nimport numpy as np\ndef dice_coeff(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dice_coeff(y_true, y_pred)\n    return loss\ndef iou_coeff(y_true, y_pred):\n    smooth=1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    union=K.sum(y_true_f) + K.sum(y_pred_f)-intersection\n    mvalue=(intersection+smooth)/(union+smooth)\n    return mvalue\ndef precision(y_true, y_pred):\n    \"\"\"Precision metric.\n\n    Only computes a batch-wise average of precision.\n\n    Computes the precision, a metric for multi-label classification of\n    how many selected items are relevant.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\ndef recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\ndef ACL5(y_true, y_pred): \n\n\t#y_pred = K.cast(y_pred, dtype = 'float64')\n\n\tprint(K.int_shape(y_pred))\n\n\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n\n\tdelta_x = x[:,1:,:-2,:]**2\n\tdelta_y = y[:,:-2,1:,:]**2\n\tdelta_u = K.abs(delta_x + delta_y) \n\n\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n\tw = 1####\n\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n\n\n\tC_1 = np.ones((256, 256))\n\tC_2 = np.zeros((256, 256))\n\n\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n\n\tlambdaP = 5 # lambda parameter could be various.\n\t\n\tloss =  lenth + lambdaP * ((region_in) + (region_out)) \n\n\treturn loss\ndef ACL5_mod(y_true, y_pred): \n\n\t#y_pred = K.cast(y_pred, dtype = 'float64')\n\n\tprint(K.int_shape(y_pred))\n\n\tx = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n\ty = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n\n\tdelta_x = x[:,1:,:-2,:]**2\n\tdelta_y = y[:,:-2,1:,:]**2\n\tdelta_u = K.abs(delta_x + delta_y) \n\n\tepsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n\tw = 1####\n\tlenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n\n\n\tC_1 = np.ones((256, 256))\n\tC_2 = np.zeros((256, 256))\n\n\tregion_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n\tregion_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n\n\tlambdaP = 5 # lambda parameter could be various.\n\t\n\tloss =  lenth + lambdaP * ((region_in) + (region_out*1.4)) \n\n\treturn loss","metadata":{"execution":{"iopub.status.busy":"2021-08-14T13:11:23.807076Z","iopub.execute_input":"2021-08-14T13:11:23.807429Z","iopub.status.idle":"2021-08-14T13:11:23.831115Z","shell.execute_reply.started":"2021-08-14T13:11:23.807394Z","shell.execute_reply":"2021-08-14T13:11:23.830270Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"beta = 0.25\nalpha = 0.25\ngamma = 2\nepsilon = 1e-5\nsmooth = 1\n\ndef tversky_index( y_true, y_pred):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth) / (true_pos + alpha * false_neg + (\n                1 - alpha) * false_pos + smooth)\n\ndef tversky_loss( y_true, y_pred):\n    return 1 - tversky_index(y_true, y_pred)\n\ndef focal_tversky( y_true, y_pred):\n    pt_1 = tversky_index(y_true, y_pred)\n    gamma = 0.75\n    return K.pow((1 - pt_1), gamma)\n\ndef dsc(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dsc(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-08-14T13:11:23.832910Z","iopub.execute_input":"2021-08-14T13:11:23.833342Z","iopub.status.idle":"2021-08-14T13:11:23.845212Z","shell.execute_reply.started":"2021-08-14T13:11:23.833308Z","shell.execute_reply":"2021-08-14T13:11:23.844456Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"%env SM_FRAMEWORK=tf.keras","metadata":{"execution":{"iopub.status.busy":"2021-08-14T13:11:23.902225Z","iopub.execute_input":"2021-08-14T13:11:23.902503Z","iopub.status.idle":"2021-08-14T13:11:23.907015Z","shell.execute_reply.started":"2021-08-14T13:11:23.902475Z","shell.execute_reply":"2021-08-14T13:11:23.906081Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"env: SM_FRAMEWORK=tf.keras\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install segmentation_models\nimport segmentation_models\nfrom segmentation_models.losses import bce_jaccard_loss","metadata":{"execution":{"iopub.status.busy":"2021-08-14T13:11:23.908804Z","iopub.execute_input":"2021-08-14T13:11:23.909369Z","iopub.status.idle":"2021-08-14T13:11:30.257797Z","shell.execute_reply.started":"2021-08-14T13:11:23.909329Z","shell.execute_reply":"2021-08-14T13:11:30.256805Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Requirement already satisfied: segmentation_models in /opt/conda/lib/python3.7/site-packages (1.0.1)\nRequirement already satisfied: image-classifiers==1.0.0 in /opt/conda/lib/python3.7/site-packages (from segmentation_models) (1.0.0)\nRequirement already satisfied: efficientnet==1.0.0 in /opt/conda/lib/python3.7/site-packages (from segmentation_models) (1.0.0)\nRequirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.7/site-packages (from segmentation_models) (1.0.8)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation_models) (0.18.2)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.19.5)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (2.10.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.15.0)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2021.7.2)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.1.1)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.5)\nRequirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.6.3)\nRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (3.4.2)\nRequirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.9.0)\nRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (8.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.8.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.4.7)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (0.10.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation_models) (5.0.9)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"def ACL5_bce_jaccard_loss(y_true, y_pred):\n    loss = ACL5(y_true, y_pred) + bce_jaccard_loss(y_true, y_pred)\n    return loss\n\n\ndef focal_tversky_bce_jaccard_loss(y_true, y_pred):\n    loss = focal_tversky(y_true, y_pred) + 2*bce_jaccard_loss(y_true, y_pred)\n    return loss\n\ndef ACL5_bce_jaccard_loss_focal_tversky(y_true, y_pred):\n    loss = ACL5(y_true, y_pred) + bce_jaccard_loss(y_true, y_pred) + focal_tversky(y_true, y_pred)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-08-14T13:11:30.261469Z","iopub.execute_input":"2021-08-14T13:11:30.261741Z","iopub.status.idle":"2021-08-14T13:11:30.268364Z","shell.execute_reply.started":"2021-08-14T13:11:30.261710Z","shell.execute_reply":"2021-08-14T13:11:30.267487Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Input, Conv2D, UpSampling2D, BatchNormalization, Activation, add,average,concatenate , Conv2DTranspose\nfrom keras.layers.core import Lambda\nfrom keras.optimizers import *\nfrom keras.losses import binary_crossentropy\n#import tensorflow as tf\n#import keras.backend as K\n\n\nIMG_SIZE = 256\nh_heuns_method=0.5\n\ndef res_block(x, nb_filters, strides):\n\n    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same' ,strides=strides[0])(x)\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Dropout(0.2)(res_path)\n    \n    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same',strides=strides[1])(res_path)\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(res_path)\n\n    shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1),strides=strides[0])(x)\n    shortcut = BatchNormalization()(shortcut)\n    shortcut = Activation(activation='relu')(shortcut)\n    shortcut = Dropout(0.2)(shortcut)\n\n    res_path = add([shortcut, hpath])#suma corta\n    return res_path\n\ndef res_block2(x,y,nb_filters, strides):\n\n    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same' ,strides=strides[0])(x)\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    res_path = Dropout(0.2)(res_path)\n    \n    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same',strides=strides[1])(res_path)\n    res_path = BatchNormalization()(res_path)\n    res_path = Activation(activation='relu')(res_path)\n    \n    hpath = Lambda(lambda x: x * h_heuns_method)(res_path)\n\n    shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1),strides=strides[0])(x)\n    shortcut = BatchNormalization()(shortcut)\n    shortcut = Activation(activation='relu')(shortcut)\n    shortcut = Dropout(0.2)(shortcut)\n    \n    res_path = add([shortcut, hpath])#suma corta\n\n    res_path = average([y, res_path])#suma doble \n    return res_path\n\n\ndef encoder(x):\n    to_decoder = []\n\n    main_path = Conv2D(filters=64, kernel_size=(3, 3), padding='same', strides=(1, 1))(x)\n    main_path = BatchNormalization()(main_path)\n    main_path = Activation(activation='relu')(main_path)\n\n    main_path = Conv2D(filters=64, kernel_size=(3, 3), padding='same', strides=(1, 1))(main_path)\n    hpath = Lambda(lambda x: x * h_heuns_method)(main_path)\n\n    shortcut = Conv2D(filters=64, kernel_size=(1, 1), strides=(1, 1))(x)\n    shortcut = BatchNormalization()(shortcut)\n\n    main_path = add([shortcut, hpath])#suma corta\n\n    to_decoder.append(main_path)\n\n\n    s1 = Conv2D(filters=128, kernel_size=(1, 1), strides=(2, 2))(x)\n    s1 = BatchNormalization()(s1)\n\n    main_path = res_block2(main_path,s1, [128, 128], [(2, 2), (1, 1)]) \n    to_decoder.append(main_path)\n\n    main_path = res_block(main_path, [256, 256], [(2, 2), (1, 1)])\n    to_decoder.append(main_path)\n\n    s2 = Conv2D(filters=512, kernel_size=(1, 1), strides=(4, 4))(to_decoder[1])\n    s2 = BatchNormalization()(s2)\n\n    main_path = res_block2(main_path,s2, [512, 512], [(2, 2), (1, 1)])\n    to_decoder.append(main_path)\n\n    return to_decoder\n\n\ndef decoder(x, from_encoder):\n## I added BatchNormalization after each Conv2DTarnspose    \n    #main_path = UpSampling2D(size=(2, 2))(x)#32x32\n    main_path = Conv2DTranspose(512, [2, 2], strides=[2, 2])(x)\n    main_path = BatchNormalization()(main_path)\n    main_path1 = concatenate([main_path, from_encoder[3]], axis=3)\n    main_path = res_block(main_path1, [512, 512], [(1, 1), (1, 1)])\n\n    #main_path = UpSampling2D(size=(2, 2))(main_path)###64x64\n    # = Conv2DTranspose(out_channel, [2, 2], strides=[2, 2])(main_path)\n    main_path = Conv2DTranspose(512, [2, 2], strides=[2, 2])(main_path)\n    main_path = BatchNormalization()(main_path)\n    main_path = concatenate([main_path, from_encoder[2]], axis=3)#\n    #u1 = UpSampling2D(size=(2, 2))(main_path1)#\n    u1 = Conv2DTranspose(256, [2, 2], strides=[2, 2])(main_path1)\n    u1 = Conv2D(256, kernel_size=(1, 1),strides=(1, 1))(u1)\n    u1 = BatchNormalization()(u1)\n    main_path = res_block2(main_path,u1, [256, 256], [(1, 1), (1, 1)])#\n\n    #main_path = UpSampling2D(size=(2, 2))(main_path)#128x128\n    main_path = Conv2DTranspose(128, [2, 2], strides=[2, 2])(main_path)\n    main_path = BatchNormalization()(main_path)\n    main_path2 = concatenate([main_path, from_encoder[1]], axis=3)#\n    main_path = res_block(main_path2, [128, 128], [(1, 1), (1, 1)])#\n\n    #main_path = UpSampling2D(size=(2, 2))(main_path)#256x256\n    main_path = Conv2DTranspose(64, [2, 2], strides=[2, 2])(main_path)\n    main_path = BatchNormalization()(main_path)\n    main_path = concatenate([main_path, from_encoder[0]], axis=3)#256x256\n\n    #u2 = UpSampling2D(size=(2,2))(main_path2)#\n    u2 = Conv2DTranspose(64, [2, 2], strides=[2, 2])(main_path2)\n    u2 = Conv2D(64, kernel_size=(1, 1),strides=(1, 1))(u2)#\n    u2 = BatchNormalization()(u2)\n    main_path = res_block2(main_path,u2, [64, 64], [(1, 1), (1, 1)])#256x256\n\n    return main_path\n\n\ndef res2unet(lrate=8.00E-05,pretrained_weights=None):\n    print(lrate)\n    input_size=(IMG_SIZE, IMG_SIZE, 3)\n    inputs = Input(shape=input_size)\n\n    to_decoder = encoder(inputs)\n\n    path = res_block(to_decoder[3], [1024, 1024], [(2, 2), (1, 1)])####bridge\n\n    path = decoder(path, from_encoder=to_decoder)\n\n\n    path = Conv2D(2, kernel_size=(3, 3),activation='relu', padding='same', strides=(1, 1))(path)\n    path = Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(path)\n    model = Model(inputs=inputs, outputs=path)\n    model.compile(optimizer=Adam(lr=lrate), loss=ACL5_bce_jaccard_loss_focal_tversky, metrics=[dice_loss,iou_coeff,precision,recall])\n    model.summary()\n    if (pretrained_weights):\n        model.load_weights(pretrained_weights)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-14T13:11:30.270025Z","iopub.execute_input":"2021-08-14T13:11:30.270325Z","iopub.status.idle":"2021-08-14T13:11:30.304382Z","shell.execute_reply.started":"2021-08-14T13:11:30.270299Z","shell.execute_reply":"2021-08-14T13:11:30.303422Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n\n#path to images\ntrain_path = \"../input/training/training\"\nimage_folder = \"images\"\nlabel_folder = \"label\"\nvalid_path =  \"../input/validation/Validation\"\nvalid_image_folder =\"images\"\nvalid_label_folder = \"label\"\nlog_filepath = './log'\nflag_multi_class = False\nnum_classes = 2\ntest_path = \"../input/testdata/Test/images\"\nsave_path = \"./\"\ndp = data_preprocess(train_path=train_path,image_folder=image_folder,label_folder=label_folder,\n                     valid_path=valid_path,valid_image_folder=valid_image_folder,valid_label_folder=valid_label_folder,\n                     flag_multi_class=flag_multi_class,\n                     num_classes=num_classes , test_path = test_path , save_path = save_path)\n\ntrain_data = dp.trainGenerator(batch_size=2)\nvalid_data = dp.validLoad(batch_size=1)\ntest_data = dp.testGenerator()\n\nmodel = res2unet(lrate=7.00E-05 )# , pretrained_weights = \"../input/weights-res2net-conv2dtranspose/Res2Net_Conv2DTranspose.hdf5\")\nlearning_rate_reduction = ReduceLROnPlateau(\n    monitor = 'val_iou_coeff', \n    patience = 2, \n    verbose = 1, \n    factor = 0.2, \n    min_lr = 0.0000001\n    )\n\nmodel_checkpoint1 = keras.callbacks.ModelCheckpoint('Res2Net.hdf5', monitor='val_dice_loss',verbose=1,mode='min',save_best_only=True)\ncsv_logger = CSVLogger('trainingRes2Net.log', append=True, separator=';')\nhistory = model.fit_generator(train_data,\n                              steps_per_epoch=1912,epochs=20,\n                              validation_steps=207,\n                              validation_data=valid_data,\n                              callbacks=[model_checkpoint1,csv_logger , learning_rate_reduction ])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-14T13:11:30.305866Z","iopub.execute_input":"2021-08-14T13:11:30.306210Z","iopub.status.idle":"2021-08-14T14:11:46.996783Z","shell.execute_reply.started":"2021-08-14T13:11:30.306165Z","shell.execute_reply":"2021-08-14T14:11:46.993477Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"7e-05\nModel: \"model_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_4 (InputLayer)            [(None, 256, 256, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_99 (Conv2D)              (None, 256, 256, 64) 1792        input_4[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_90 (BatchNo (None, 256, 256, 64) 256         conv2d_99[0][0]                  \n__________________________________________________________________________________________________\nactivation_75 (Activation)      (None, 256, 256, 64) 0           batch_normalization_90[0][0]     \n__________________________________________________________________________________________________\nconv2d_101 (Conv2D)             (None, 256, 256, 64) 256         input_4[0][0]                    \n__________________________________________________________________________________________________\nconv2d_100 (Conv2D)             (None, 256, 256, 64) 36928       activation_75[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_91 (BatchNo (None, 256, 256, 64) 256         conv2d_101[0][0]                 \n__________________________________________________________________________________________________\nlambda_27 (Lambda)              (None, 256, 256, 64) 0           conv2d_100[0][0]                 \n__________________________________________________________________________________________________\nadd_27 (Add)                    (None, 256, 256, 64) 0           batch_normalization_91[0][0]     \n                                                                 lambda_27[0][0]                  \n__________________________________________________________________________________________________\nconv2d_103 (Conv2D)             (None, 128, 128, 128 73856       add_27[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_93 (BatchNo (None, 128, 128, 128 512         conv2d_103[0][0]                 \n__________________________________________________________________________________________________\nactivation_76 (Activation)      (None, 128, 128, 128 0           batch_normalization_93[0][0]     \n__________________________________________________________________________________________________\ndropout_48 (Dropout)            (None, 128, 128, 128 0           activation_76[0][0]              \n__________________________________________________________________________________________________\nconv2d_105 (Conv2D)             (None, 128, 128, 128 8320        add_27[0][0]                     \n__________________________________________________________________________________________________\nconv2d_104 (Conv2D)             (None, 128, 128, 128 147584      dropout_48[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_95 (BatchNo (None, 128, 128, 128 512         conv2d_105[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_94 (BatchNo (None, 128, 128, 128 512         conv2d_104[0][0]                 \n__________________________________________________________________________________________________\nactivation_78 (Activation)      (None, 128, 128, 128 0           batch_normalization_95[0][0]     \n__________________________________________________________________________________________________\nactivation_77 (Activation)      (None, 128, 128, 128 0           batch_normalization_94[0][0]     \n__________________________________________________________________________________________________\nconv2d_102 (Conv2D)             (None, 128, 128, 128 512         input_4[0][0]                    \n__________________________________________________________________________________________________\ndropout_49 (Dropout)            (None, 128, 128, 128 0           activation_78[0][0]              \n__________________________________________________________________________________________________\nlambda_28 (Lambda)              (None, 128, 128, 128 0           activation_77[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_92 (BatchNo (None, 128, 128, 128 512         conv2d_102[0][0]                 \n__________________________________________________________________________________________________\nadd_28 (Add)                    (None, 128, 128, 128 0           dropout_49[0][0]                 \n                                                                 lambda_28[0][0]                  \n__________________________________________________________________________________________________\naverage_12 (Average)            (None, 128, 128, 128 0           batch_normalization_92[0][0]     \n                                                                 add_28[0][0]                     \n__________________________________________________________________________________________________\nconv2d_106 (Conv2D)             (None, 64, 64, 256)  295168      average_12[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_96 (BatchNo (None, 64, 64, 256)  1024        conv2d_106[0][0]                 \n__________________________________________________________________________________________________\nactivation_79 (Activation)      (None, 64, 64, 256)  0           batch_normalization_96[0][0]     \n__________________________________________________________________________________________________\ndropout_50 (Dropout)            (None, 64, 64, 256)  0           activation_79[0][0]              \n__________________________________________________________________________________________________\nconv2d_108 (Conv2D)             (None, 64, 64, 256)  33024       average_12[0][0]                 \n__________________________________________________________________________________________________\nconv2d_107 (Conv2D)             (None, 64, 64, 256)  590080      dropout_50[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_98 (BatchNo (None, 64, 64, 256)  1024        conv2d_108[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_97 (BatchNo (None, 64, 64, 256)  1024        conv2d_107[0][0]                 \n__________________________________________________________________________________________________\nactivation_81 (Activation)      (None, 64, 64, 256)  0           batch_normalization_98[0][0]     \n__________________________________________________________________________________________________\nactivation_80 (Activation)      (None, 64, 64, 256)  0           batch_normalization_97[0][0]     \n__________________________________________________________________________________________________\ndropout_51 (Dropout)            (None, 64, 64, 256)  0           activation_81[0][0]              \n__________________________________________________________________________________________________\nlambda_29 (Lambda)              (None, 64, 64, 256)  0           activation_80[0][0]              \n__________________________________________________________________________________________________\nadd_29 (Add)                    (None, 64, 64, 256)  0           dropout_51[0][0]                 \n                                                                 lambda_29[0][0]                  \n__________________________________________________________________________________________________\nconv2d_110 (Conv2D)             (None, 32, 32, 512)  1180160     add_29[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_100 (BatchN (None, 32, 32, 512)  2048        conv2d_110[0][0]                 \n__________________________________________________________________________________________________\nactivation_82 (Activation)      (None, 32, 32, 512)  0           batch_normalization_100[0][0]    \n__________________________________________________________________________________________________\ndropout_52 (Dropout)            (None, 32, 32, 512)  0           activation_82[0][0]              \n__________________________________________________________________________________________________\nconv2d_112 (Conv2D)             (None, 32, 32, 512)  131584      add_29[0][0]                     \n__________________________________________________________________________________________________\nconv2d_111 (Conv2D)             (None, 32, 32, 512)  2359808     dropout_52[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_102 (BatchN (None, 32, 32, 512)  2048        conv2d_112[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_101 (BatchN (None, 32, 32, 512)  2048        conv2d_111[0][0]                 \n__________________________________________________________________________________________________\nactivation_84 (Activation)      (None, 32, 32, 512)  0           batch_normalization_102[0][0]    \n__________________________________________________________________________________________________\nactivation_83 (Activation)      (None, 32, 32, 512)  0           batch_normalization_101[0][0]    \n__________________________________________________________________________________________________\nconv2d_109 (Conv2D)             (None, 32, 32, 512)  66048       average_12[0][0]                 \n__________________________________________________________________________________________________\ndropout_53 (Dropout)            (None, 32, 32, 512)  0           activation_84[0][0]              \n__________________________________________________________________________________________________\nlambda_30 (Lambda)              (None, 32, 32, 512)  0           activation_83[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_99 (BatchNo (None, 32, 32, 512)  2048        conv2d_109[0][0]                 \n__________________________________________________________________________________________________\nadd_30 (Add)                    (None, 32, 32, 512)  0           dropout_53[0][0]                 \n                                                                 lambda_30[0][0]                  \n__________________________________________________________________________________________________\naverage_13 (Average)            (None, 32, 32, 512)  0           batch_normalization_99[0][0]     \n                                                                 add_30[0][0]                     \n__________________________________________________________________________________________________\nconv2d_113 (Conv2D)             (None, 16, 16, 1024) 4719616     average_13[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_103 (BatchN (None, 16, 16, 1024) 4096        conv2d_113[0][0]                 \n__________________________________________________________________________________________________\nactivation_85 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_103[0][0]    \n__________________________________________________________________________________________________\ndropout_54 (Dropout)            (None, 16, 16, 1024) 0           activation_85[0][0]              \n__________________________________________________________________________________________________\nconv2d_115 (Conv2D)             (None, 16, 16, 1024) 525312      average_13[0][0]                 \n__________________________________________________________________________________________________\nconv2d_114 (Conv2D)             (None, 16, 16, 1024) 9438208     dropout_54[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_105 (BatchN (None, 16, 16, 1024) 4096        conv2d_115[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_104 (BatchN (None, 16, 16, 1024) 4096        conv2d_114[0][0]                 \n__________________________________________________________________________________________________\nactivation_87 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_105[0][0]    \n__________________________________________________________________________________________________\nactivation_86 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_104[0][0]    \n__________________________________________________________________________________________________\ndropout_55 (Dropout)            (None, 16, 16, 1024) 0           activation_87[0][0]              \n__________________________________________________________________________________________________\nlambda_31 (Lambda)              (None, 16, 16, 1024) 0           activation_86[0][0]              \n__________________________________________________________________________________________________\nadd_31 (Add)                    (None, 16, 16, 1024) 0           dropout_55[0][0]                 \n                                                                 lambda_31[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_18 (Conv2DTran (None, 32, 32, 512)  2097664     add_31[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_106 (BatchN (None, 32, 32, 512)  2048        conv2d_transpose_18[0][0]        \n__________________________________________________________________________________________________\nconcatenate_12 (Concatenate)    (None, 32, 32, 1024) 0           batch_normalization_106[0][0]    \n                                                                 average_13[0][0]                 \n__________________________________________________________________________________________________\nconv2d_116 (Conv2D)             (None, 32, 32, 512)  4719104     concatenate_12[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_107 (BatchN (None, 32, 32, 512)  2048        conv2d_116[0][0]                 \n__________________________________________________________________________________________________\nactivation_88 (Activation)      (None, 32, 32, 512)  0           batch_normalization_107[0][0]    \n__________________________________________________________________________________________________\ndropout_56 (Dropout)            (None, 32, 32, 512)  0           activation_88[0][0]              \n__________________________________________________________________________________________________\nconv2d_118 (Conv2D)             (None, 32, 32, 512)  524800      concatenate_12[0][0]             \n__________________________________________________________________________________________________\nconv2d_117 (Conv2D)             (None, 32, 32, 512)  2359808     dropout_56[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_109 (BatchN (None, 32, 32, 512)  2048        conv2d_118[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_108 (BatchN (None, 32, 32, 512)  2048        conv2d_117[0][0]                 \n__________________________________________________________________________________________________\nactivation_90 (Activation)      (None, 32, 32, 512)  0           batch_normalization_109[0][0]    \n__________________________________________________________________________________________________\nactivation_89 (Activation)      (None, 32, 32, 512)  0           batch_normalization_108[0][0]    \n__________________________________________________________________________________________________\ndropout_57 (Dropout)            (None, 32, 32, 512)  0           activation_90[0][0]              \n__________________________________________________________________________________________________\nlambda_32 (Lambda)              (None, 32, 32, 512)  0           activation_89[0][0]              \n__________________________________________________________________________________________________\nadd_32 (Add)                    (None, 32, 32, 512)  0           dropout_57[0][0]                 \n                                                                 lambda_32[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_19 (Conv2DTran (None, 64, 64, 512)  1049088     add_32[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_110 (BatchN (None, 64, 64, 512)  2048        conv2d_transpose_19[0][0]        \n__________________________________________________________________________________________________\nconcatenate_13 (Concatenate)    (None, 64, 64, 768)  0           batch_normalization_110[0][0]    \n                                                                 add_29[0][0]                     \n__________________________________________________________________________________________________\nconv2d_120 (Conv2D)             (None, 64, 64, 256)  1769728     concatenate_13[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_112 (BatchN (None, 64, 64, 256)  1024        conv2d_120[0][0]                 \n__________________________________________________________________________________________________\nactivation_91 (Activation)      (None, 64, 64, 256)  0           batch_normalization_112[0][0]    \n__________________________________________________________________________________________________\ndropout_58 (Dropout)            (None, 64, 64, 256)  0           activation_91[0][0]              \n__________________________________________________________________________________________________\nconv2d_122 (Conv2D)             (None, 64, 64, 256)  196864      concatenate_13[0][0]             \n__________________________________________________________________________________________________\nconv2d_121 (Conv2D)             (None, 64, 64, 256)  590080      dropout_58[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_114 (BatchN (None, 64, 64, 256)  1024        conv2d_122[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_113 (BatchN (None, 64, 64, 256)  1024        conv2d_121[0][0]                 \n__________________________________________________________________________________________________\nconv2d_transpose_20 (Conv2DTran (None, 64, 64, 256)  1048832     concatenate_12[0][0]             \n__________________________________________________________________________________________________\nactivation_93 (Activation)      (None, 64, 64, 256)  0           batch_normalization_114[0][0]    \n__________________________________________________________________________________________________\nactivation_92 (Activation)      (None, 64, 64, 256)  0           batch_normalization_113[0][0]    \n__________________________________________________________________________________________________\nconv2d_119 (Conv2D)             (None, 64, 64, 256)  65792       conv2d_transpose_20[0][0]        \n__________________________________________________________________________________________________\ndropout_59 (Dropout)            (None, 64, 64, 256)  0           activation_93[0][0]              \n__________________________________________________________________________________________________\nlambda_33 (Lambda)              (None, 64, 64, 256)  0           activation_92[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_111 (BatchN (None, 64, 64, 256)  1024        conv2d_119[0][0]                 \n__________________________________________________________________________________________________\nadd_33 (Add)                    (None, 64, 64, 256)  0           dropout_59[0][0]                 \n                                                                 lambda_33[0][0]                  \n__________________________________________________________________________________________________\naverage_14 (Average)            (None, 64, 64, 256)  0           batch_normalization_111[0][0]    \n                                                                 add_33[0][0]                     \n__________________________________________________________________________________________________\nconv2d_transpose_21 (Conv2DTran (None, 128, 128, 128 131200      average_14[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_115 (BatchN (None, 128, 128, 128 512         conv2d_transpose_21[0][0]        \n__________________________________________________________________________________________________\nconcatenate_14 (Concatenate)    (None, 128, 128, 256 0           batch_normalization_115[0][0]    \n                                                                 average_12[0][0]                 \n__________________________________________________________________________________________________\nconv2d_123 (Conv2D)             (None, 128, 128, 128 295040      concatenate_14[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_116 (BatchN (None, 128, 128, 128 512         conv2d_123[0][0]                 \n__________________________________________________________________________________________________\nactivation_94 (Activation)      (None, 128, 128, 128 0           batch_normalization_116[0][0]    \n__________________________________________________________________________________________________\ndropout_60 (Dropout)            (None, 128, 128, 128 0           activation_94[0][0]              \n__________________________________________________________________________________________________\nconv2d_125 (Conv2D)             (None, 128, 128, 128 32896       concatenate_14[0][0]             \n__________________________________________________________________________________________________\nconv2d_124 (Conv2D)             (None, 128, 128, 128 147584      dropout_60[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_118 (BatchN (None, 128, 128, 128 512         conv2d_125[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_117 (BatchN (None, 128, 128, 128 512         conv2d_124[0][0]                 \n__________________________________________________________________________________________________\nactivation_96 (Activation)      (None, 128, 128, 128 0           batch_normalization_118[0][0]    \n__________________________________________________________________________________________________\nactivation_95 (Activation)      (None, 128, 128, 128 0           batch_normalization_117[0][0]    \n__________________________________________________________________________________________________\ndropout_61 (Dropout)            (None, 128, 128, 128 0           activation_96[0][0]              \n__________________________________________________________________________________________________\nlambda_34 (Lambda)              (None, 128, 128, 128 0           activation_95[0][0]              \n__________________________________________________________________________________________________\nadd_34 (Add)                    (None, 128, 128, 128 0           dropout_61[0][0]                 \n                                                                 lambda_34[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_22 (Conv2DTran (None, 256, 256, 64) 32832       add_34[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_119 (BatchN (None, 256, 256, 64) 256         conv2d_transpose_22[0][0]        \n__________________________________________________________________________________________________\nconcatenate_15 (Concatenate)    (None, 256, 256, 128 0           batch_normalization_119[0][0]    \n                                                                 add_27[0][0]                     \n__________________________________________________________________________________________________\nconv2d_127 (Conv2D)             (None, 256, 256, 64) 73792       concatenate_15[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_121 (BatchN (None, 256, 256, 64) 256         conv2d_127[0][0]                 \n__________________________________________________________________________________________________\nactivation_97 (Activation)      (None, 256, 256, 64) 0           batch_normalization_121[0][0]    \n__________________________________________________________________________________________________\ndropout_62 (Dropout)            (None, 256, 256, 64) 0           activation_97[0][0]              \n__________________________________________________________________________________________________\nconv2d_129 (Conv2D)             (None, 256, 256, 64) 8256        concatenate_15[0][0]             \n__________________________________________________________________________________________________\nconv2d_128 (Conv2D)             (None, 256, 256, 64) 36928       dropout_62[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_123 (BatchN (None, 256, 256, 64) 256         conv2d_129[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_122 (BatchN (None, 256, 256, 64) 256         conv2d_128[0][0]                 \n__________________________________________________________________________________________________\nconv2d_transpose_23 (Conv2DTran (None, 256, 256, 64) 65600       concatenate_14[0][0]             \n__________________________________________________________________________________________________\nactivation_99 (Activation)      (None, 256, 256, 64) 0           batch_normalization_123[0][0]    \n__________________________________________________________________________________________________\nactivation_98 (Activation)      (None, 256, 256, 64) 0           batch_normalization_122[0][0]    \n__________________________________________________________________________________________________\nconv2d_126 (Conv2D)             (None, 256, 256, 64) 4160        conv2d_transpose_23[0][0]        \n__________________________________________________________________________________________________\ndropout_63 (Dropout)            (None, 256, 256, 64) 0           activation_99[0][0]              \n__________________________________________________________________________________________________\nlambda_35 (Lambda)              (None, 256, 256, 64) 0           activation_98[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_120 (BatchN (None, 256, 256, 64) 256         conv2d_126[0][0]                 \n__________________________________________________________________________________________________\nadd_35 (Add)                    (None, 256, 256, 64) 0           dropout_63[0][0]                 \n                                                                 lambda_35[0][0]                  \n__________________________________________________________________________________________________\naverage_15 (Average)            (None, 256, 256, 64) 0           batch_normalization_120[0][0]    \n                                                                 add_35[0][0]                     \n__________________________________________________________________________________________________\nconv2d_130 (Conv2D)             (None, 256, 256, 2)  1154        average_15[0][0]                 \n__________________________________________________________________________________________________\nconv2d_131 (Conv2D)             (None, 256, 256, 1)  3           conv2d_130[0][0]                 \n==================================================================================================\nTotal params: 34,903,237\nTrainable params: 34,881,349\nNon-trainable params: 21,888\n__________________________________________________________________________________________________\nFound 600 images belonging to 1 classes.\nFound 600 images belonging to 1 classes.\nEpoch 1/20\n(None, 256, 256, 1)\n(None, 256, 256, 1)\n1912/1912 [==============================] - ETA: 0s - loss: 44843.1891 - dice_loss: 0.7925 - iou_coeff: 0.1236 - precision: 0.2469 - recall: 0.0128Found 200 images belonging to 1 classes.\nFound 200 images belonging to 1 classes.\n(None, 256, 256, 1)\n1912/1912 [==============================] - 262s 134ms/step - loss: 44829.5803 - dice_loss: 0.7924 - iou_coeff: 0.1236 - precision: 0.2470 - recall: 0.0128 - val_loss: 3962.9150 - val_dice_loss: 0.5962 - val_iou_coeff: 0.2659 - val_precision: 0.0048 - val_recall: 4.1971e-06\n\nEpoch 00001: val_dice_loss improved from inf to 0.59622, saving model to Res2Net.hdf5\nEpoch 2/20\n1912/1912 [==============================] - 254s 133ms/step - loss: 7449.1027 - dice_loss: 0.5867 - iou_coeff: 0.2648 - precision: 0.3175 - recall: 2.9214e-04 - val_loss: 3628.7161 - val_dice_loss: 0.6067 - val_iou_coeff: 0.2610 - val_precision: 0.1014 - val_recall: 1.1926e-04\n\nEpoch 00002: val_dice_loss did not improve from 0.59622\nEpoch 3/20\n1912/1912 [==============================] - 255s 134ms/step - loss: 7036.7773 - dice_loss: 0.5732 - iou_coeff: 0.2768 - precision: 0.6293 - recall: 0.3794 - val_loss: 3453.9163 - val_dice_loss: 0.5927 - val_iou_coeff: 0.2805 - val_precision: 0.6688 - val_recall: 0.5460\n\nEpoch 00003: val_dice_loss improved from 0.59622 to 0.59267, saving model to Res2Net.hdf5\nEpoch 4/20\n1912/1912 [==============================] - 254s 133ms/step - loss: 6832.0520 - dice_loss: 0.5443 - iou_coeff: 0.3001 - precision: 0.8003 - recall: 0.6304 - val_loss: 5754.8208 - val_dice_loss: 0.6200 - val_iou_coeff: 0.2433 - val_precision: 0.4778 - val_recall: 0.6535\n\nEpoch 00004: val_dice_loss did not improve from 0.59267\nEpoch 5/20\n1912/1912 [==============================] - 255s 134ms/step - loss: 6516.1877 - dice_loss: 0.5081 - iou_coeff: 0.3320 - precision: 0.8191 - recall: 0.6598 - val_loss: 3431.2241 - val_dice_loss: 0.4917 - val_iou_coeff: 0.3615 - val_precision: 0.6770 - val_recall: 0.7112\n\nEpoch 00005: val_dice_loss improved from 0.59267 to 0.49172, saving model to Res2Net.hdf5\nEpoch 6/20\n1912/1912 [==============================] - 256s 134ms/step - loss: 6328.8316 - dice_loss: 0.4819 - iou_coeff: 0.3562 - precision: 0.8287 - recall: 0.6761 - val_loss: 3167.9658 - val_dice_loss: 0.4784 - val_iou_coeff: 0.3803 - val_precision: 0.7265 - val_recall: 0.6296\n\nEpoch 00006: val_dice_loss improved from 0.49172 to 0.47835, saving model to Res2Net.hdf5\n\nEpoch 00006: ReduceLROnPlateau reducing learning rate to 1.4000000373926015e-05.\nEpoch 7/20\n1912/1912 [==============================] - 257s 134ms/step - loss: 5889.8834 - dice_loss: 0.4346 - iou_coeff: 0.3988 - precision: 0.8476 - recall: 0.7245 - val_loss: 3070.8962 - val_dice_loss: 0.5114 - val_iou_coeff: 0.3597 - val_precision: 0.7184 - val_recall: 0.5551\n\nEpoch 00007: val_dice_loss did not improve from 0.47835\nEpoch 8/20\n1912/1912 [==============================] - 256s 134ms/step - loss: 5739.3327 - dice_loss: 0.4236 - iou_coeff: 0.4101 - precision: 0.8569 - recall: 0.7343 - val_loss: 3038.2300 - val_dice_loss: 0.4358 - val_iou_coeff: 0.4161 - val_precision: 0.7723 - val_recall: 0.6911\n\nEpoch 00008: val_dice_loss improved from 0.47835 to 0.43580, saving model to Res2Net.hdf5\n\nEpoch 00008: ReduceLROnPlateau reducing learning rate to 2.8000000384054148e-06.\nEpoch 9/20\n1912/1912 [==============================] - 257s 135ms/step - loss: 5675.7478 - dice_loss: 0.4142 - iou_coeff: 0.4189 - precision: 0.8599 - recall: 0.7447 - val_loss: 2886.2849 - val_dice_loss: 0.4476 - val_iou_coeff: 0.4115 - val_precision: 0.7449 - val_recall: 0.6532\n\nEpoch 00009: val_dice_loss did not improve from 0.43580\nEpoch 10/20\n1912/1912 [==============================] - 256s 134ms/step - loss: 5601.3652 - dice_loss: 0.4076 - iou_coeff: 0.4252 - precision: 0.8618 - recall: 0.7545 - val_loss: 3046.7361 - val_dice_loss: 0.4442 - val_iou_coeff: 0.4114 - val_precision: 0.7665 - val_recall: 0.6505\n\nEpoch 00010: val_dice_loss did not improve from 0.43580\n\nEpoch 00010: ReduceLROnPlateau reducing learning rate to 5.599999894911889e-07.\nEpoch 11/20\n1912/1912 [==============================] - 257s 134ms/step - loss: 5636.5444 - dice_loss: 0.4092 - iou_coeff: 0.4244 - precision: 0.8558 - recall: 0.7527 - val_loss: 2915.2759 - val_dice_loss: 0.4341 - val_iou_coeff: 0.4187 - val_precision: 0.7722 - val_recall: 0.6878\n\nEpoch 00011: val_dice_loss improved from 0.43580 to 0.43412, saving model to Res2Net.hdf5\nEpoch 12/20\n1912/1912 [==============================] - 257s 134ms/step - loss: 5634.4608 - dice_loss: 0.4092 - iou_coeff: 0.4242 - precision: 0.8575 - recall: 0.7506 - val_loss: 2861.7705 - val_dice_loss: 0.4474 - val_iou_coeff: 0.4130 - val_precision: 0.7434 - val_recall: 0.6361\n\nEpoch 00012: val_dice_loss did not improve from 0.43412\n\nEpoch 00012: ReduceLROnPlateau reducing learning rate to 1.1199999789823778e-07.\nEpoch 13/20\n1912/1912 [==============================] - 256s 134ms/step - loss: 5638.2920 - dice_loss: 0.4062 - iou_coeff: 0.4263 - precision: 0.8603 - recall: 0.7559 - val_loss: 3037.6479 - val_dice_loss: 0.4397 - val_iou_coeff: 0.4114 - val_precision: 0.7974 - val_recall: 0.6799\n\nEpoch 00013: val_dice_loss did not improve from 0.43412\nEpoch 14/20\n1912/1912 [==============================] - 256s 134ms/step - loss: 5620.0077 - dice_loss: 0.4096 - iou_coeff: 0.4235 - precision: 0.8585 - recall: 0.7492 - val_loss: 2932.3589 - val_dice_loss: 0.4490 - val_iou_coeff: 0.4096 - val_precision: 0.7520 - val_recall: 0.6444\n\nEpoch 00014: val_dice_loss did not improve from 0.43412\n\nEpoch 00014: ReduceLROnPlateau reducing learning rate to 1e-07.\nEpoch 15/20\n 135/1912 [=>............................] - ETA: 3:56 - loss: 5886.6797 - dice_loss: 0.4222 - iou_coeff: 0.4122 - precision: 0.8551 - recall: 0.7326","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-1d63d4f1d7e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m207\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                               callbacks=[model_checkpoint1,csv_logger , learning_rate_reduction ])\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   def evaluate_generator(self,\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model = res2unet(lrate=7.00E-05 , pretrained_weights = './Res2Net.hdf5')\n\n\nhistory = model.fit_generator(train_data,\n                              steps_per_epoch=1912,epochs=30,\n                              validation_steps=207,\n                              validation_data=valid_data,\n                              callbacks=[model_checkpoint1,csv_logger ])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:12:15.165423Z","iopub.execute_input":"2021-08-14T14:12:15.165785Z","iopub.status.idle":"2021-08-14T16:20:30.321309Z","shell.execute_reply.started":"2021-08-14T14:12:15.165752Z","shell.execute_reply":"2021-08-14T16:20:30.320385Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"7e-05\nModel: \"model_4\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_5 (InputLayer)            [(None, 256, 256, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_132 (Conv2D)             (None, 256, 256, 64) 1792        input_5[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_124 (BatchN (None, 256, 256, 64) 256         conv2d_132[0][0]                 \n__________________________________________________________________________________________________\nactivation_100 (Activation)     (None, 256, 256, 64) 0           batch_normalization_124[0][0]    \n__________________________________________________________________________________________________\nconv2d_134 (Conv2D)             (None, 256, 256, 64) 256         input_5[0][0]                    \n__________________________________________________________________________________________________\nconv2d_133 (Conv2D)             (None, 256, 256, 64) 36928       activation_100[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_125 (BatchN (None, 256, 256, 64) 256         conv2d_134[0][0]                 \n__________________________________________________________________________________________________\nlambda_36 (Lambda)              (None, 256, 256, 64) 0           conv2d_133[0][0]                 \n__________________________________________________________________________________________________\nadd_36 (Add)                    (None, 256, 256, 64) 0           batch_normalization_125[0][0]    \n                                                                 lambda_36[0][0]                  \n__________________________________________________________________________________________________\nconv2d_136 (Conv2D)             (None, 128, 128, 128 73856       add_36[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_127 (BatchN (None, 128, 128, 128 512         conv2d_136[0][0]                 \n__________________________________________________________________________________________________\nactivation_101 (Activation)     (None, 128, 128, 128 0           batch_normalization_127[0][0]    \n__________________________________________________________________________________________________\ndropout_64 (Dropout)            (None, 128, 128, 128 0           activation_101[0][0]             \n__________________________________________________________________________________________________\nconv2d_138 (Conv2D)             (None, 128, 128, 128 8320        add_36[0][0]                     \n__________________________________________________________________________________________________\nconv2d_137 (Conv2D)             (None, 128, 128, 128 147584      dropout_64[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_129 (BatchN (None, 128, 128, 128 512         conv2d_138[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_128 (BatchN (None, 128, 128, 128 512         conv2d_137[0][0]                 \n__________________________________________________________________________________________________\nactivation_103 (Activation)     (None, 128, 128, 128 0           batch_normalization_129[0][0]    \n__________________________________________________________________________________________________\nactivation_102 (Activation)     (None, 128, 128, 128 0           batch_normalization_128[0][0]    \n__________________________________________________________________________________________________\nconv2d_135 (Conv2D)             (None, 128, 128, 128 512         input_5[0][0]                    \n__________________________________________________________________________________________________\ndropout_65 (Dropout)            (None, 128, 128, 128 0           activation_103[0][0]             \n__________________________________________________________________________________________________\nlambda_37 (Lambda)              (None, 128, 128, 128 0           activation_102[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_126 (BatchN (None, 128, 128, 128 512         conv2d_135[0][0]                 \n__________________________________________________________________________________________________\nadd_37 (Add)                    (None, 128, 128, 128 0           dropout_65[0][0]                 \n                                                                 lambda_37[0][0]                  \n__________________________________________________________________________________________________\naverage_16 (Average)            (None, 128, 128, 128 0           batch_normalization_126[0][0]    \n                                                                 add_37[0][0]                     \n__________________________________________________________________________________________________\nconv2d_139 (Conv2D)             (None, 64, 64, 256)  295168      average_16[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_130 (BatchN (None, 64, 64, 256)  1024        conv2d_139[0][0]                 \n__________________________________________________________________________________________________\nactivation_104 (Activation)     (None, 64, 64, 256)  0           batch_normalization_130[0][0]    \n__________________________________________________________________________________________________\ndropout_66 (Dropout)            (None, 64, 64, 256)  0           activation_104[0][0]             \n__________________________________________________________________________________________________\nconv2d_141 (Conv2D)             (None, 64, 64, 256)  33024       average_16[0][0]                 \n__________________________________________________________________________________________________\nconv2d_140 (Conv2D)             (None, 64, 64, 256)  590080      dropout_66[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_132 (BatchN (None, 64, 64, 256)  1024        conv2d_141[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_131 (BatchN (None, 64, 64, 256)  1024        conv2d_140[0][0]                 \n__________________________________________________________________________________________________\nactivation_106 (Activation)     (None, 64, 64, 256)  0           batch_normalization_132[0][0]    \n__________________________________________________________________________________________________\nactivation_105 (Activation)     (None, 64, 64, 256)  0           batch_normalization_131[0][0]    \n__________________________________________________________________________________________________\ndropout_67 (Dropout)            (None, 64, 64, 256)  0           activation_106[0][0]             \n__________________________________________________________________________________________________\nlambda_38 (Lambda)              (None, 64, 64, 256)  0           activation_105[0][0]             \n__________________________________________________________________________________________________\nadd_38 (Add)                    (None, 64, 64, 256)  0           dropout_67[0][0]                 \n                                                                 lambda_38[0][0]                  \n__________________________________________________________________________________________________\nconv2d_143 (Conv2D)             (None, 32, 32, 512)  1180160     add_38[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_134 (BatchN (None, 32, 32, 512)  2048        conv2d_143[0][0]                 \n__________________________________________________________________________________________________\nactivation_107 (Activation)     (None, 32, 32, 512)  0           batch_normalization_134[0][0]    \n__________________________________________________________________________________________________\ndropout_68 (Dropout)            (None, 32, 32, 512)  0           activation_107[0][0]             \n__________________________________________________________________________________________________\nconv2d_145 (Conv2D)             (None, 32, 32, 512)  131584      add_38[0][0]                     \n__________________________________________________________________________________________________\nconv2d_144 (Conv2D)             (None, 32, 32, 512)  2359808     dropout_68[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_136 (BatchN (None, 32, 32, 512)  2048        conv2d_145[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_135 (BatchN (None, 32, 32, 512)  2048        conv2d_144[0][0]                 \n__________________________________________________________________________________________________\nactivation_109 (Activation)     (None, 32, 32, 512)  0           batch_normalization_136[0][0]    \n__________________________________________________________________________________________________\nactivation_108 (Activation)     (None, 32, 32, 512)  0           batch_normalization_135[0][0]    \n__________________________________________________________________________________________________\nconv2d_142 (Conv2D)             (None, 32, 32, 512)  66048       average_16[0][0]                 \n__________________________________________________________________________________________________\ndropout_69 (Dropout)            (None, 32, 32, 512)  0           activation_109[0][0]             \n__________________________________________________________________________________________________\nlambda_39 (Lambda)              (None, 32, 32, 512)  0           activation_108[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_133 (BatchN (None, 32, 32, 512)  2048        conv2d_142[0][0]                 \n__________________________________________________________________________________________________\nadd_39 (Add)                    (None, 32, 32, 512)  0           dropout_69[0][0]                 \n                                                                 lambda_39[0][0]                  \n__________________________________________________________________________________________________\naverage_17 (Average)            (None, 32, 32, 512)  0           batch_normalization_133[0][0]    \n                                                                 add_39[0][0]                     \n__________________________________________________________________________________________________\nconv2d_146 (Conv2D)             (None, 16, 16, 1024) 4719616     average_17[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_137 (BatchN (None, 16, 16, 1024) 4096        conv2d_146[0][0]                 \n__________________________________________________________________________________________________\nactivation_110 (Activation)     (None, 16, 16, 1024) 0           batch_normalization_137[0][0]    \n__________________________________________________________________________________________________\ndropout_70 (Dropout)            (None, 16, 16, 1024) 0           activation_110[0][0]             \n__________________________________________________________________________________________________\nconv2d_148 (Conv2D)             (None, 16, 16, 1024) 525312      average_17[0][0]                 \n__________________________________________________________________________________________________\nconv2d_147 (Conv2D)             (None, 16, 16, 1024) 9438208     dropout_70[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_139 (BatchN (None, 16, 16, 1024) 4096        conv2d_148[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_138 (BatchN (None, 16, 16, 1024) 4096        conv2d_147[0][0]                 \n__________________________________________________________________________________________________\nactivation_112 (Activation)     (None, 16, 16, 1024) 0           batch_normalization_139[0][0]    \n__________________________________________________________________________________________________\nactivation_111 (Activation)     (None, 16, 16, 1024) 0           batch_normalization_138[0][0]    \n__________________________________________________________________________________________________\ndropout_71 (Dropout)            (None, 16, 16, 1024) 0           activation_112[0][0]             \n__________________________________________________________________________________________________\nlambda_40 (Lambda)              (None, 16, 16, 1024) 0           activation_111[0][0]             \n__________________________________________________________________________________________________\nadd_40 (Add)                    (None, 16, 16, 1024) 0           dropout_71[0][0]                 \n                                                                 lambda_40[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_24 (Conv2DTran (None, 32, 32, 512)  2097664     add_40[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_140 (BatchN (None, 32, 32, 512)  2048        conv2d_transpose_24[0][0]        \n__________________________________________________________________________________________________\nconcatenate_16 (Concatenate)    (None, 32, 32, 1024) 0           batch_normalization_140[0][0]    \n                                                                 average_17[0][0]                 \n__________________________________________________________________________________________________\nconv2d_149 (Conv2D)             (None, 32, 32, 512)  4719104     concatenate_16[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_141 (BatchN (None, 32, 32, 512)  2048        conv2d_149[0][0]                 \n__________________________________________________________________________________________________\nactivation_113 (Activation)     (None, 32, 32, 512)  0           batch_normalization_141[0][0]    \n__________________________________________________________________________________________________\ndropout_72 (Dropout)            (None, 32, 32, 512)  0           activation_113[0][0]             \n__________________________________________________________________________________________________\nconv2d_151 (Conv2D)             (None, 32, 32, 512)  524800      concatenate_16[0][0]             \n__________________________________________________________________________________________________\nconv2d_150 (Conv2D)             (None, 32, 32, 512)  2359808     dropout_72[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_143 (BatchN (None, 32, 32, 512)  2048        conv2d_151[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_142 (BatchN (None, 32, 32, 512)  2048        conv2d_150[0][0]                 \n__________________________________________________________________________________________________\nactivation_115 (Activation)     (None, 32, 32, 512)  0           batch_normalization_143[0][0]    \n__________________________________________________________________________________________________\nactivation_114 (Activation)     (None, 32, 32, 512)  0           batch_normalization_142[0][0]    \n__________________________________________________________________________________________________\ndropout_73 (Dropout)            (None, 32, 32, 512)  0           activation_115[0][0]             \n__________________________________________________________________________________________________\nlambda_41 (Lambda)              (None, 32, 32, 512)  0           activation_114[0][0]             \n__________________________________________________________________________________________________\nadd_41 (Add)                    (None, 32, 32, 512)  0           dropout_73[0][0]                 \n                                                                 lambda_41[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_25 (Conv2DTran (None, 64, 64, 512)  1049088     add_41[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_144 (BatchN (None, 64, 64, 512)  2048        conv2d_transpose_25[0][0]        \n__________________________________________________________________________________________________\nconcatenate_17 (Concatenate)    (None, 64, 64, 768)  0           batch_normalization_144[0][0]    \n                                                                 add_38[0][0]                     \n__________________________________________________________________________________________________\nconv2d_153 (Conv2D)             (None, 64, 64, 256)  1769728     concatenate_17[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_146 (BatchN (None, 64, 64, 256)  1024        conv2d_153[0][0]                 \n__________________________________________________________________________________________________\nactivation_116 (Activation)     (None, 64, 64, 256)  0           batch_normalization_146[0][0]    \n__________________________________________________________________________________________________\ndropout_74 (Dropout)            (None, 64, 64, 256)  0           activation_116[0][0]             \n__________________________________________________________________________________________________\nconv2d_155 (Conv2D)             (None, 64, 64, 256)  196864      concatenate_17[0][0]             \n__________________________________________________________________________________________________\nconv2d_154 (Conv2D)             (None, 64, 64, 256)  590080      dropout_74[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_148 (BatchN (None, 64, 64, 256)  1024        conv2d_155[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_147 (BatchN (None, 64, 64, 256)  1024        conv2d_154[0][0]                 \n__________________________________________________________________________________________________\nconv2d_transpose_26 (Conv2DTran (None, 64, 64, 256)  1048832     concatenate_16[0][0]             \n__________________________________________________________________________________________________\nactivation_118 (Activation)     (None, 64, 64, 256)  0           batch_normalization_148[0][0]    \n__________________________________________________________________________________________________\nactivation_117 (Activation)     (None, 64, 64, 256)  0           batch_normalization_147[0][0]    \n__________________________________________________________________________________________________\nconv2d_152 (Conv2D)             (None, 64, 64, 256)  65792       conv2d_transpose_26[0][0]        \n__________________________________________________________________________________________________\ndropout_75 (Dropout)            (None, 64, 64, 256)  0           activation_118[0][0]             \n__________________________________________________________________________________________________\nlambda_42 (Lambda)              (None, 64, 64, 256)  0           activation_117[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_145 (BatchN (None, 64, 64, 256)  1024        conv2d_152[0][0]                 \n__________________________________________________________________________________________________\nadd_42 (Add)                    (None, 64, 64, 256)  0           dropout_75[0][0]                 \n                                                                 lambda_42[0][0]                  \n__________________________________________________________________________________________________\naverage_18 (Average)            (None, 64, 64, 256)  0           batch_normalization_145[0][0]    \n                                                                 add_42[0][0]                     \n__________________________________________________________________________________________________\nconv2d_transpose_27 (Conv2DTran (None, 128, 128, 128 131200      average_18[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_149 (BatchN (None, 128, 128, 128 512         conv2d_transpose_27[0][0]        \n__________________________________________________________________________________________________\nconcatenate_18 (Concatenate)    (None, 128, 128, 256 0           batch_normalization_149[0][0]    \n                                                                 average_16[0][0]                 \n__________________________________________________________________________________________________\nconv2d_156 (Conv2D)             (None, 128, 128, 128 295040      concatenate_18[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_150 (BatchN (None, 128, 128, 128 512         conv2d_156[0][0]                 \n__________________________________________________________________________________________________\nactivation_119 (Activation)     (None, 128, 128, 128 0           batch_normalization_150[0][0]    \n__________________________________________________________________________________________________\ndropout_76 (Dropout)            (None, 128, 128, 128 0           activation_119[0][0]             \n__________________________________________________________________________________________________\nconv2d_158 (Conv2D)             (None, 128, 128, 128 32896       concatenate_18[0][0]             \n__________________________________________________________________________________________________\nconv2d_157 (Conv2D)             (None, 128, 128, 128 147584      dropout_76[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_152 (BatchN (None, 128, 128, 128 512         conv2d_158[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_151 (BatchN (None, 128, 128, 128 512         conv2d_157[0][0]                 \n__________________________________________________________________________________________________\nactivation_121 (Activation)     (None, 128, 128, 128 0           batch_normalization_152[0][0]    \n__________________________________________________________________________________________________\nactivation_120 (Activation)     (None, 128, 128, 128 0           batch_normalization_151[0][0]    \n__________________________________________________________________________________________________\ndropout_77 (Dropout)            (None, 128, 128, 128 0           activation_121[0][0]             \n__________________________________________________________________________________________________\nlambda_43 (Lambda)              (None, 128, 128, 128 0           activation_120[0][0]             \n__________________________________________________________________________________________________\nadd_43 (Add)                    (None, 128, 128, 128 0           dropout_77[0][0]                 \n                                                                 lambda_43[0][0]                  \n__________________________________________________________________________________________________\nconv2d_transpose_28 (Conv2DTran (None, 256, 256, 64) 32832       add_43[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_153 (BatchN (None, 256, 256, 64) 256         conv2d_transpose_28[0][0]        \n__________________________________________________________________________________________________\nconcatenate_19 (Concatenate)    (None, 256, 256, 128 0           batch_normalization_153[0][0]    \n                                                                 add_36[0][0]                     \n__________________________________________________________________________________________________\nconv2d_160 (Conv2D)             (None, 256, 256, 64) 73792       concatenate_19[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_155 (BatchN (None, 256, 256, 64) 256         conv2d_160[0][0]                 \n__________________________________________________________________________________________________\nactivation_122 (Activation)     (None, 256, 256, 64) 0           batch_normalization_155[0][0]    \n__________________________________________________________________________________________________\ndropout_78 (Dropout)            (None, 256, 256, 64) 0           activation_122[0][0]             \n__________________________________________________________________________________________________\nconv2d_162 (Conv2D)             (None, 256, 256, 64) 8256        concatenate_19[0][0]             \n__________________________________________________________________________________________________\nconv2d_161 (Conv2D)             (None, 256, 256, 64) 36928       dropout_78[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_157 (BatchN (None, 256, 256, 64) 256         conv2d_162[0][0]                 \n__________________________________________________________________________________________________\nbatch_normalization_156 (BatchN (None, 256, 256, 64) 256         conv2d_161[0][0]                 \n__________________________________________________________________________________________________\nconv2d_transpose_29 (Conv2DTran (None, 256, 256, 64) 65600       concatenate_18[0][0]             \n__________________________________________________________________________________________________\nactivation_124 (Activation)     (None, 256, 256, 64) 0           batch_normalization_157[0][0]    \n__________________________________________________________________________________________________\nactivation_123 (Activation)     (None, 256, 256, 64) 0           batch_normalization_156[0][0]    \n__________________________________________________________________________________________________\nconv2d_159 (Conv2D)             (None, 256, 256, 64) 4160        conv2d_transpose_29[0][0]        \n__________________________________________________________________________________________________\ndropout_79 (Dropout)            (None, 256, 256, 64) 0           activation_124[0][0]             \n__________________________________________________________________________________________________\nlambda_44 (Lambda)              (None, 256, 256, 64) 0           activation_123[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_154 (BatchN (None, 256, 256, 64) 256         conv2d_159[0][0]                 \n__________________________________________________________________________________________________\nadd_44 (Add)                    (None, 256, 256, 64) 0           dropout_79[0][0]                 \n                                                                 lambda_44[0][0]                  \n__________________________________________________________________________________________________\naverage_19 (Average)            (None, 256, 256, 64) 0           batch_normalization_154[0][0]    \n                                                                 add_44[0][0]                     \n__________________________________________________________________________________________________\nconv2d_163 (Conv2D)             (None, 256, 256, 2)  1154        average_19[0][0]                 \n__________________________________________________________________________________________________\nconv2d_164 (Conv2D)             (None, 256, 256, 1)  3           conv2d_163[0][0]                 \n==================================================================================================\nTotal params: 34,903,237\nTrainable params: 34,881,349\nNon-trainable params: 21,888\n__________________________________________________________________________________________________\nEpoch 1/30\n(None, 256, 256, 1)\n(None, 256, 256, 1)\n1912/1912 [==============================] - ETA: 0s - loss: 5822.9717 - dice_loss: 0.4302 - iou_coeff: 0.4043 - precision: 0.8445 - recall: 0.7119(None, 256, 256, 1)\n1912/1912 [==============================] - 261s 134ms/step - loss: 5822.9735 - dice_loss: 0.4302 - iou_coeff: 0.4043 - precision: 0.8445 - recall: 0.7119 - val_loss: 3107.0686 - val_dice_loss: 0.4417 - val_iou_coeff: 0.4116 - val_precision: 0.7296 - val_recall: 0.6614\n\nEpoch 00001: val_dice_loss did not improve from 0.43412\nEpoch 2/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 5655.1313 - dice_loss: 0.4069 - iou_coeff: 0.4278 - precision: 0.8459 - recall: 0.7243 - val_loss: 2815.2524 - val_dice_loss: 0.4100 - val_iou_coeff: 0.4384 - val_precision: 0.8120 - val_recall: 0.6901\n\nEpoch 00002: val_dice_loss improved from 0.43412 to 0.40996, saving model to Res2Net.hdf5\nEpoch 3/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 5502.1819 - dice_loss: 0.3906 - iou_coeff: 0.4451 - precision: 0.8508 - recall: 0.7235 - val_loss: 2862.5930 - val_dice_loss: 0.3690 - val_iou_coeff: 0.4859 - val_precision: 0.7655 - val_recall: 0.6930\n\nEpoch 00003: val_dice_loss improved from 0.40996 to 0.36900, saving model to Res2Net.hdf5\nEpoch 4/30\n1912/1912 [==============================] - 255s 134ms/step - loss: 5323.8452 - dice_loss: 0.3658 - iou_coeff: 0.4708 - precision: 0.8529 - recall: 0.7396 - val_loss: 3304.0562 - val_dice_loss: 0.6523 - val_iou_coeff: 0.2726 - val_precision: 0.5077 - val_recall: 0.3079\n\nEpoch 00004: val_dice_loss did not improve from 0.36900\nEpoch 5/30\n1912/1912 [==============================] - 255s 133ms/step - loss: 5145.6533 - dice_loss: 0.3488 - iou_coeff: 0.4901 - precision: 0.8535 - recall: 0.7444 - val_loss: 2780.2109 - val_dice_loss: 0.3795 - val_iou_coeff: 0.4794 - val_precision: 0.7894 - val_recall: 0.6735\n\nEpoch 00005: val_dice_loss did not improve from 0.36900\nEpoch 6/30\n1912/1912 [==============================] - 255s 133ms/step - loss: 5019.8246 - dice_loss: 0.3366 - iou_coeff: 0.5038 - precision: 0.8557 - recall: 0.7428 - val_loss: 2855.3188 - val_dice_loss: 0.4667 - val_iou_coeff: 0.4223 - val_precision: 0.6799 - val_recall: 0.5132\n\nEpoch 00006: val_dice_loss did not improve from 0.36900\nEpoch 7/30\n1912/1912 [==============================] - 255s 134ms/step - loss: 4834.5052 - dice_loss: 0.3227 - iou_coeff: 0.5201 - precision: 0.8571 - recall: 0.7456 - val_loss: 2465.4578 - val_dice_loss: 0.3319 - val_iou_coeff: 0.5318 - val_precision: 0.7929 - val_recall: 0.6967\n\nEpoch 00007: val_dice_loss improved from 0.36900 to 0.33194, saving model to Res2Net.hdf5\nEpoch 8/30\n1912/1912 [==============================] - 255s 134ms/step - loss: 4673.6306 - dice_loss: 0.3013 - iou_coeff: 0.5439 - precision: 0.8620 - recall: 0.7584 - val_loss: 2822.6272 - val_dice_loss: 0.4524 - val_iou_coeff: 0.4397 - val_precision: 0.6416 - val_recall: 0.5429\n\nEpoch 00008: val_dice_loss did not improve from 0.33194\nEpoch 9/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 4590.0242 - dice_loss: 0.2916 - iou_coeff: 0.5567 - precision: 0.8616 - recall: 0.7595 - val_loss: 2883.8931 - val_dice_loss: 0.4871 - val_iou_coeff: 0.4069 - val_precision: 0.7061 - val_recall: 0.4464\n\nEpoch 00009: val_dice_loss did not improve from 0.33194\nEpoch 10/30\n1912/1912 [==============================] - 255s 134ms/step - loss: 4454.3139 - dice_loss: 0.2775 - iou_coeff: 0.5734 - precision: 0.8609 - recall: 0.7640 - val_loss: 2640.9329 - val_dice_loss: 0.4036 - val_iou_coeff: 0.4815 - val_precision: 0.7324 - val_recall: 0.5493\n\nEpoch 00010: val_dice_loss did not improve from 0.33194\nEpoch 11/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 4352.8226 - dice_loss: 0.2698 - iou_coeff: 0.5839 - precision: 0.8627 - recall: 0.7662 - val_loss: 2338.6216 - val_dice_loss: 0.2878 - val_iou_coeff: 0.5757 - val_precision: 0.7943 - val_recall: 0.7287\n\nEpoch 00011: val_dice_loss improved from 0.33194 to 0.28777, saving model to Res2Net.hdf5\nEpoch 12/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 4250.2675 - dice_loss: 0.2567 - iou_coeff: 0.5989 - precision: 0.8657 - recall: 0.7743 - val_loss: 2305.5100 - val_dice_loss: 0.2921 - val_iou_coeff: 0.5823 - val_precision: 0.7921 - val_recall: 0.7126\n\nEpoch 00012: val_dice_loss did not improve from 0.28777\nEpoch 13/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 4065.8376 - dice_loss: 0.2444 - iou_coeff: 0.6147 - precision: 0.8687 - recall: 0.7803 - val_loss: 2685.7214 - val_dice_loss: 0.4140 - val_iou_coeff: 0.4763 - val_precision: 0.7193 - val_recall: 0.5249\n\nEpoch 00013: val_dice_loss did not improve from 0.28777\nEpoch 14/30\n1912/1912 [==============================] - 257s 134ms/step - loss: 4017.1725 - dice_loss: 0.2391 - iou_coeff: 0.6220 - precision: 0.8696 - recall: 0.7789 - val_loss: 2171.3892 - val_dice_loss: 0.3017 - val_iou_coeff: 0.5805 - val_precision: 0.7811 - val_recall: 0.6698\n\nEpoch 00014: val_dice_loss did not improve from 0.28777\nEpoch 15/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 3900.6507 - dice_loss: 0.2276 - iou_coeff: 0.6359 - precision: 0.8702 - recall: 0.7879 - val_loss: 2297.4263 - val_dice_loss: 0.2901 - val_iou_coeff: 0.5873 - val_precision: 0.7687 - val_recall: 0.6882\n\nEpoch 00015: val_dice_loss did not improve from 0.28777\nEpoch 16/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 3865.6032 - dice_loss: 0.2260 - iou_coeff: 0.6400 - precision: 0.8684 - recall: 0.7819 - val_loss: 2126.1597 - val_dice_loss: 0.2974 - val_iou_coeff: 0.5900 - val_precision: 0.7703 - val_recall: 0.6717\n\nEpoch 00016: val_dice_loss did not improve from 0.28777\nEpoch 17/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 3673.8494 - dice_loss: 0.2132 - iou_coeff: 0.6566 - precision: 0.8696 - recall: 0.7961 - val_loss: 2371.4224 - val_dice_loss: 0.3719 - val_iou_coeff: 0.5212 - val_precision: 0.7312 - val_recall: 0.5617\n\nEpoch 00017: val_dice_loss did not improve from 0.28777\nEpoch 18/30\n1912/1912 [==============================] - 255s 134ms/step - loss: 3644.4683 - dice_loss: 0.2061 - iou_coeff: 0.6651 - precision: 0.8742 - recall: 0.7980 - val_loss: 2184.0769 - val_dice_loss: 0.2674 - val_iou_coeff: 0.6137 - val_precision: 0.7768 - val_recall: 0.6961\n\nEpoch 00018: val_dice_loss improved from 0.28777 to 0.26744, saving model to Res2Net.hdf5\nEpoch 19/30\n1912/1912 [==============================] - 255s 134ms/step - loss: 3643.9494 - dice_loss: 0.2054 - iou_coeff: 0.6673 - precision: 0.8656 - recall: 0.7977 - val_loss: 2184.6140 - val_dice_loss: 0.3040 - val_iou_coeff: 0.5843 - val_precision: 0.7803 - val_recall: 0.6658\n\nEpoch 00019: val_dice_loss did not improve from 0.26744\nEpoch 20/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 3577.1949 - dice_loss: 0.2010 - iou_coeff: 0.6743 - precision: 0.8698 - recall: 0.7922 - val_loss: 1980.0487 - val_dice_loss: 0.2450 - val_iou_coeff: 0.6406 - val_precision: 0.7968 - val_recall: 0.6897\n\nEpoch 00020: val_dice_loss improved from 0.26744 to 0.24502, saving model to Res2Net.hdf5\nEpoch 21/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 3544.4482 - dice_loss: 0.2017 - iou_coeff: 0.6740 - precision: 0.8702 - recall: 0.7917 - val_loss: 2185.0417 - val_dice_loss: 0.3288 - val_iou_coeff: 0.5626 - val_precision: 0.7546 - val_recall: 0.6176\n\nEpoch 00021: val_dice_loss did not improve from 0.24502\nEpoch 22/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 3402.9639 - dice_loss: 0.1893 - iou_coeff: 0.6897 - precision: 0.8683 - recall: 0.8028 - val_loss: 2142.3745 - val_dice_loss: 0.2739 - val_iou_coeff: 0.6126 - val_precision: 0.7563 - val_recall: 0.7347\n\nEpoch 00022: val_dice_loss did not improve from 0.24502\nEpoch 23/30\n1912/1912 [==============================] - 255s 133ms/step - loss: 3400.6840 - dice_loss: 0.1861 - iou_coeff: 0.6938 - precision: 0.8712 - recall: 0.8042 - val_loss: 2120.1443 - val_dice_loss: 0.2861 - val_iou_coeff: 0.6109 - val_precision: 0.7517 - val_recall: 0.6361\n\nEpoch 00023: val_dice_loss did not improve from 0.24502\nEpoch 24/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 3304.8105 - dice_loss: 0.1793 - iou_coeff: 0.7030 - precision: 0.8759 - recall: 0.8105 - val_loss: 2040.4808 - val_dice_loss: 0.2272 - val_iou_coeff: 0.6573 - val_precision: 0.7853 - val_recall: 0.7491\n\nEpoch 00024: val_dice_loss improved from 0.24502 to 0.22719, saving model to Res2Net.hdf5\nEpoch 25/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 3266.3599 - dice_loss: 0.1745 - iou_coeff: 0.7094 - precision: 0.8757 - recall: 0.8109 - val_loss: 2145.6140 - val_dice_loss: 0.3292 - val_iou_coeff: 0.5767 - val_precision: 0.7209 - val_recall: 0.5780\n\nEpoch 00025: val_dice_loss did not improve from 0.22719\nEpoch 26/30\n1912/1912 [==============================] - 258s 135ms/step - loss: 3305.5164 - dice_loss: 0.1807 - iou_coeff: 0.7019 - precision: 0.8732 - recall: 0.8049 - val_loss: 2437.8562 - val_dice_loss: 0.2923 - val_iou_coeff: 0.5918 - val_precision: 0.7365 - val_recall: 0.6917\n\nEpoch 00026: val_dice_loss did not improve from 0.22719\nEpoch 27/30\n1912/1912 [==============================] - 258s 135ms/step - loss: 3308.7247 - dice_loss: 0.1792 - iou_coeff: 0.7040 - precision: 0.8742 - recall: 0.8046 - val_loss: 2099.2795 - val_dice_loss: 0.3010 - val_iou_coeff: 0.5986 - val_precision: 0.7226 - val_recall: 0.6340\n\nEpoch 00027: val_dice_loss did not improve from 0.22719\nEpoch 28/30\n1912/1912 [==============================] - 257s 134ms/step - loss: 3195.9177 - dice_loss: 0.1713 - iou_coeff: 0.7145 - precision: 0.8775 - recall: 0.8118 - val_loss: 2283.4468 - val_dice_loss: 0.3688 - val_iou_coeff: 0.5433 - val_precision: 0.6628 - val_recall: 0.5540\n\nEpoch 00028: val_dice_loss did not improve from 0.22719\nEpoch 29/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 3144.5948 - dice_loss: 0.1687 - iou_coeff: 0.7178 - precision: 0.8785 - recall: 0.8153 - val_loss: 1787.8409 - val_dice_loss: 0.2263 - val_iou_coeff: 0.6643 - val_precision: 0.8293 - val_recall: 0.7234\n\nEpoch 00029: val_dice_loss improved from 0.22719 to 0.22627, saving model to Res2Net.hdf5\nEpoch 30/30\n1912/1912 [==============================] - 256s 134ms/step - loss: 3100.0505 - dice_loss: 0.1671 - iou_coeff: 0.7218 - precision: 0.8739 - recall: 0.8116 - val_loss: 1937.6930 - val_dice_loss: 0.2230 - val_iou_coeff: 0.6753 - val_precision: 0.7730 - val_recall: 0.7379\n\nEpoch 00030: val_dice_loss improved from 0.22627 to 0.22301, saving model to Res2Net.hdf5\n","output_type":"stream"}]},{"cell_type":"code","source":"#model = res2unet(lrate=7.00E-05 , pretrained_weights = './Res2Net.hdf5')\n\n\n#model_checkpoint1 = keras.callbacks.ModelCheckpoint(\"../input/weights-res2net-conv2dtranspose/Res2Net_Conv2DTranspose.hdf5\", monitor='val_dice_loss',verbose=1,mode='min',save_best_only=True)\n#csv_logger = CSVLogger('trainingRes2Net.log', append=True, separator=';')\nhistory = model.fit_generator(train_data,\n                             steps_per_epoch=1912,epochs=10,\n                              validation_steps=207,\n                              validation_data=valid_data,\n                              callbacks=[model_checkpoint1,csv_logger])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-14T16:32:03.956374Z","iopub.execute_input":"2021-08-14T16:32:03.956755Z","iopub.status.idle":"2021-08-14T17:12:12.664476Z","shell.execute_reply.started":"2021-08-14T16:32:03.956721Z","shell.execute_reply":"2021-08-14T17:12:12.661415Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1912/1912 [==============================] - 256s 134ms/step - loss: 3068.2207 - dice_loss: 0.1627 - iou_coeff: 0.7265 - precision: 0.8799 - recall: 0.8188 - val_loss: 1874.8134 - val_dice_loss: 0.2235 - val_iou_coeff: 0.6684 - val_precision: 0.8028 - val_recall: 0.7278\n\nEpoch 00001: val_dice_loss did not improve from 0.22301\nEpoch 2/10\n1912/1912 [==============================] - 256s 134ms/step - loss: 3041.9524 - dice_loss: 0.1588 - iou_coeff: 0.7312 - precision: 0.8821 - recall: 0.8219 - val_loss: 2007.9214 - val_dice_loss: 0.2744 - val_iou_coeff: 0.6273 - val_precision: 0.7413 - val_recall: 0.6545\n\nEpoch 00002: val_dice_loss did not improve from 0.22301\nEpoch 3/10\n1912/1912 [==============================] - 255s 133ms/step - loss: 3015.0010 - dice_loss: 0.1591 - iou_coeff: 0.7318 - precision: 0.8828 - recall: 0.8217 - val_loss: 1871.2096 - val_dice_loss: 0.2294 - val_iou_coeff: 0.6637 - val_precision: 0.7983 - val_recall: 0.6789\n\nEpoch 00003: val_dice_loss did not improve from 0.22301\nEpoch 4/10\n1912/1912 [==============================] - 255s 134ms/step - loss: 3021.3711 - dice_loss: 0.1585 - iou_coeff: 0.7326 - precision: 0.8812 - recall: 0.8201 - val_loss: 1909.9744 - val_dice_loss: 0.2575 - val_iou_coeff: 0.6423 - val_precision: 0.7883 - val_recall: 0.6828\n\nEpoch 00004: val_dice_loss did not improve from 0.22301\nEpoch 5/10\n1912/1912 [==============================] - 256s 134ms/step - loss: 2968.6667 - dice_loss: 0.1559 - iou_coeff: 0.7370 - precision: 0.8793 - recall: 0.8227 - val_loss: 1941.4648 - val_dice_loss: 0.2592 - val_iou_coeff: 0.6381 - val_precision: 0.7687 - val_recall: 0.7111\n\nEpoch 00005: val_dice_loss did not improve from 0.22301\nEpoch 6/10\n1912/1912 [==============================] - 256s 134ms/step - loss: 2992.1174 - dice_loss: 0.1568 - iou_coeff: 0.7358 - precision: 0.8829 - recall: 0.8216 - val_loss: 1859.3438 - val_dice_loss: 0.2279 - val_iou_coeff: 0.6678 - val_precision: 0.7733 - val_recall: 0.7343\n\nEpoch 00006: val_dice_loss did not improve from 0.22301\nEpoch 7/10\n1912/1912 [==============================] - 256s 134ms/step - loss: 2938.2888 - dice_loss: 0.1534 - iou_coeff: 0.7408 - precision: 0.8808 - recall: 0.8223 - val_loss: 1898.2594 - val_dice_loss: 0.2381 - val_iou_coeff: 0.6585 - val_precision: 0.7872 - val_recall: 0.7162\n\nEpoch 00007: val_dice_loss did not improve from 0.22301\nEpoch 8/10\n1912/1912 [==============================] - 255s 134ms/step - loss: 2926.0461 - dice_loss: 0.1517 - iou_coeff: 0.7427 - precision: 0.8821 - recall: 0.8266 - val_loss: 1884.7721 - val_dice_loss: 0.2436 - val_iou_coeff: 0.6591 - val_precision: 0.7734 - val_recall: 0.6943\n\nEpoch 00008: val_dice_loss did not improve from 0.22301\nEpoch 9/10\n1912/1912 [==============================] - 256s 134ms/step - loss: 2894.0750 - dice_loss: 0.1503 - iou_coeff: 0.7449 - precision: 0.8851 - recall: 0.8283 - val_loss: 2013.3693 - val_dice_loss: 0.2942 - val_iou_coeff: 0.6078 - val_precision: 0.7507 - val_recall: 0.6222\n\nEpoch 00009: val_dice_loss did not improve from 0.22301\nEpoch 10/10\n 829/1912 [============>.................] - ETA: 2:21 - loss: 2907.4312 - dice_loss: 0.1509 - iou_coeff: 0.7438 - precision: 0.8849 - recall: 0.8274","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-6207d3d15ea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m207\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                               callbacks=[model_checkpoint1,csv_logger])\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   def evaluate_generator(self,\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"history = model.fit_generator(train_data,\n                             steps_per_epoch=1912,epochs=10,\n                              validation_steps=207,\n                              validation_data=valid_data,\n                              callbacks=[model_checkpoint1,csv_logger])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-14T17:12:12.667204Z","iopub.status.idle":"2021-08-14T17:12:12.669435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#path to images\ntrain_path = \"../input/training/training\"\nimage_folder = \"images\"\nlabel_folder = \"label\"\nvalid_path =  \"../input/validation/Validation\"\nvalid_image_folder =\"images\"\nvalid_label_folder = \"label\"\nlog_filepath = './log'\nflag_multi_class = False\nnum_classes = 2\ntest_path = \"../input/testdata/Test/images\"\nsave_path = \"./\"\ndp = data_preprocess(train_path=train_path,image_folder=image_folder,label_folder=label_folder,\n                     valid_path=valid_path,valid_image_folder=valid_image_folder,valid_label_folder=valid_label_folder,\n                     flag_multi_class=flag_multi_class,\n                     num_classes=num_classes , test_path = test_path , save_path = save_path)\n\ntrain_data = dp.trainGenerator(batch_size=2)\nvalid_data = dp.validLoad(batch_size=1)\ntest_data = dp.testGenerator()","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:11:47.003986Z","iopub.status.idle":"2021-08-14T14:11:47.004748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred = model.predict(test_data)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:11:47.005860Z","iopub.status.idle":"2021-08-14T14:11:47.006621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(ypred[1])","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:11:47.007747Z","iopub.status.idle":"2021-08-14T14:11:47.008510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(ypred[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:11:47.009614Z","iopub.status.idle":"2021-08-14T14:11:47.010385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/testdata/Test'\ntest_image_folder =\"images\"\nTest_path = test_path + '/' + 'test_image_folder'\ntest_label_folder =\"label\"\n\n\nimport os\n  \n# Directory\ndirectory = \"RESULT_Res2Net_Conv2DTransposeLayers1\"\n  \n# Parent Directory path\nparent_dir = \"./\"\n  \n# Path\npath = os.path.join(parent_dir, directory)\nos.mkdir(path)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:11:47.011641Z","iopub.status.idle":"2021-08-14T14:11:47.012374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(ypred[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:11:47.013495Z","iopub.status.idle":"2021-08-14T14:11:47.014235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nif __name__ == \"__main__\":\n    ## Dataset\n    \n    #batch_size = 8\n    #(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(\"/content/drive/MyDrive/Test\")\n    test_x = os.listdir(os.path.join(test_path ,test_image_folder ))\n    test_y = os.listdir(os.path.join(test_path ,test_label_folder ))\n    \n    TEST_X = []\n    TEST_Y = []\n    for img_file_name in test_x:\n        TEST_X.append('../input/testdata/Test/images' + \"/\" +img_file_name)\n        \n    for label_file_name in test_y:\n        TEST_Y.append('../input/testdata/Test/label'+ \"/\" +label_file_name)\n    #test_dataset = tf_dataset( TEST_X , TEST_Y, batch=batch_size)\n\n    #test_steps = (len(TEST_X)//batch_size)\n    #if len(TEST_X) % batch_size != 0:\n    #    test_steps += 1\n\n    #with CustomObjectScope({'iou': iou}):\n    #    model = tf.keras.models.load_model(\"/content/drive/model.h5\")\n\n    #model.evaluate(test_dataset, steps=test_steps)\n\n    for i, (x, y) in tqdm(enumerate(zip(TEST_X, TEST_Y)), total=len(TEST_X)):\n        x = io.imread(x , as_gray = True)\n        x = trans.resize(x, (256, 256), mode='constant')\n        #x = x[: , : , :3]\n        y = io.imread(y , as_gray = True )\n        y = trans.resize(y, (256 , 256), mode='constant')\n        \n        #y = y[: , : , :3]\n        #print(x.shape)\n        #print(y.shape)\n        #print(x.shape)\n        #print(ypred[i].reshape)\n        pred_img = np.reshape(ypred[i], (256 , 256))\n        #y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w = x.shape\n        #plt.imshow(pred_img* 255.0)\n        #break\n        white_line = np.ones((h, 10)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            y * 255.0, white_line,\n            pred_img * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        \n        cv2.imwrite(f\"./RESULT_Res2Net_Conv2DTransposeLayers1/{i}.png\", image)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:11:47.015388Z","iopub.status.idle":"2021-08-14T14:11:47.016159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nfrom shutil import make_archive\nshutil.make_archive(\"./RESULT_Res2Net_Conv2DTransposeLayers1\", 'zip', \"./RESULT_Res2Net_Conv2DTransposeLayers1\")\n","metadata":{"execution":{"iopub.status.busy":"2021-08-14T14:11:47.017385Z","iopub.status.idle":"2021-08-14T14:11:47.018206Z"},"trusted":true},"execution_count":null,"outputs":[]}]}